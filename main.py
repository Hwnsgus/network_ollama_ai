import pdfplumber
import pandas as pd
import json
import urllib.parse
import os
import sys
import argparse
import re
import math
import datetime
import ollama

# ==========================================
# â˜… 0. ë¡œê·¸ ë° ì¶œë ¥ í´ë˜ìŠ¤
# ==========================================
class DualWriter:
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "w", encoding="utf-8")

    def write(self, message):
        try:
            self.terminal.write(message)
            self.log.write(message)
            self.flush()
        except Exception:
            pass

    def flush(self):
        try:
            self.terminal.flush()
            self.log.flush()
        except Exception:
            pass

# ==========================================
# â˜… 1. ë‚´ë¶€ ì œí’ˆ DB ë¡œë“œ í•¨ìˆ˜ (RAG ê¸°ì´ˆ)
# ==========================================
def load_internal_db():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    db_path = os.path.join(script_dir, "internal_products.json")

    if not os.path.exists(db_path):
        print(f"[System] 'internal_products.json' íŒŒì¼ì´ ì—†ì–´ ì™¸ë¶€ ì œí’ˆë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        # ë””ë²„ê¹…ìš©: ì–´ë””ì„œ ì°¾ìœ¼ë ¤ë‹¤ ì‹¤íŒ¨í–ˆëŠ”ì§€ ë³´ì—¬ì¤Œ
        # print(f"   (ì°¾ëŠ” ìœ„ì¹˜: {db_path})") 
        return "No internal product list found. Proceed with external deduction only."
    
    try:
        with open(db_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            db_text = ""
            for item in data:
                # AIê°€ ì½ê¸° í¸í•œ í¬ë§·ìœ¼ë¡œ ë³€í™˜
                db_text += f"- [OUR STOCK] Category: {item.get('category','')} | Maker: {item.get('maker','')} | Model: {item.get('model','')} | Specs: {item.get('specs','')}\n"
            return db_text
    except Exception as e:
        print(f"[Warning] ë‚´ë¶€ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "Error loading internal database."

# ==========================================
# â˜… 2. Ollama í˜¸ì¶œ í•¨ìˆ˜
# ==========================================
def run_ollama_chat(model_name, prompt):
    full_response = ""
    print(f"\n      â–¼ [AI ì‹¤ì‹œê°„ ë‹µë³€ ì‹œì‘] â–¼")
    print("-" * 40)

    try:
        stream = ollama.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': prompt}],
            options={
                'num_ctx': 8192,  # ê¸´ ë¬¸ë§¥ ê¸°ì–µ
                'temperature': 0.1, # ì •í™•ë„ ìš°ì„  (ì°½ì˜ì„± ë‚®ì¶¤)
            },
            stream=True
        )

        for chunk in stream:
            content = chunk['message']['content']
            print(content, end="", flush=True)
            full_response += content

        print("\n" + "-" * 40)
        print(f"      â–² [AI ë‹µë³€ ì™„ë£Œ] â–²\n")
        return full_response

    except Exception as e:
        print(f"\n[System Error] Ollama í†µì‹  ì‹¤íŒ¨: {e}")
        return None

# ==========================================
# â˜… 3. JSON ì •ì œ í•¨ìˆ˜
# ==========================================
def clean_json_output(raw_text):
    raw_text = re.sub(r'```json\s*', '', raw_text)
    raw_text = re.sub(r'```', '', raw_text)
    match = re.search(r'(\{.*\}|\[.*\])', raw_text, re.DOTALL)
    if match:
        return match.group(0)
    return raw_text

# ==========================================
# â˜… 4. PDF ì²˜ë¦¬ ë° AI ë¶„ì„ (í•µì‹¬ ë¡œì§)
# ==========================================
def process_pdf(pdf_path, model_name):
    # 1. ë‚´ë¶€ DB ë¡œë“œ
    internal_products_str = load_internal_db()
    print(f"[System] ìì‚¬/í˜‘ë ¥ì‚¬ ì œí’ˆ DB ì¤€ë¹„ ì™„ë£Œ.")

    # 2. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
    pages_content = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            print(f"[System] '{os.path.basename(pdf_path)}' ë¡œë”© ì¤‘... (ì´ {len(pdf.pages)}í˜ì´ì§€)")
            
            start_parsing = False
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if not text: continue
                
                # ê·œê²©ì„œ ì‹œì‘/ë ê°ì§€ ë¡œì§
                if "ë¬¼í’ˆê·œê²©ì„œ" in text.replace(" ", "") or "Commodity Description" in text:
                    start_parsing = True
                if "ë³„ì§€" in text and "ì„œì‹" in text:
                    start_parsing = False
                
                if start_parsing:
                    tables = page.extract_tables()
                    if tables or "í’ˆëª…" in text or "Specifications" in text:
                        pages_content.append(text)

    except Exception as e:
        print(f"[Error] PDF ì½ê¸° ì‹¤íŒ¨: {e}")
        return []

    if not pages_content:
        print("[Warning] íŠ¹ì • ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•´ ì „ì²´ í˜ì´ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
        with pdfplumber.open(pdf_path) as pdf:
            pages_content = [p.extract_text() for p in pdf.pages if p.extract_text()]

    # 3. ë°°ì¹˜ ì²˜ë¦¬ (3í˜ì´ì§€ì”© ëŠì–´ì„œ ë¶„ì„)
    BATCH_SIZE = 3
    total_batches = math.ceil(len(pages_content) / BATCH_SIZE)
    all_items = []
    
    print(f"[System] ë¶„ì„ ëŒ€ìƒ: {len(pages_content)}í˜ì´ì§€ / ì´ {total_batches}íšŒ ì‹¤í–‰")

    for i in range(total_batches):
        start_idx = i * BATCH_SIZE
        end_idx = min((i + 1) * BATCH_SIZE, len(pages_content))
        
        chunk_pages = pages_content[start_idx:end_idx]
        chunk_text = "\n".join(chunk_pages)
        
        if len(chunk_text.strip()) < 50: continue

        print(f"   â”” [ì§„í–‰] {i+1}/{total_batches}ë²ˆì§¸ ë¬¶ìŒ ë¶„ì„ ì¤‘ (ìì‚¬ DB ëŒ€ì¡° + ìŠ¤í™ ì—­ì¶”ì )...")

        # â–¼ [í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡¬í”„íŠ¸] â–¼
        prompt = f"""
        Role: You are a Senior Pre-Sales Engineer for Pro AV & IT equipment.
        Task: Analyze the anonymous specifications and identify the EXACT product model.

        [STEP 1: CHECK INTERNAL INVENTORY (PRIORITY)]
        First, verify if any item in the request matches our Internal Product List below.
        If a match is found based on category and key specs, **YOU MUST SELECT THE INTERNAL PRODUCT**.
        
        >>> INTERNAL PRODUCT LIST <<<
        {internal_products_str}
        >>> END OF LIST <<<

        [STEP 2: IF NO INTERNAL MATCH -> DEDUCE EXTERNAL MODEL]
        Only if the item is NOT in our internal list, perform "Reverse Engineering" to find the original external brand.
        
        [RULES FOR DEDUCTION (EXTERNAL ITEMS)]
        1. **Analyze Specs**: Look for unique identifiers.
        2. **Find the Original**: Match specs against major brands.
        3. **Pricing Accuracy (CRITICAL)**: ALWAYS estimate the MSRP or market price in USD ($) first, because your training data is mostly in English. Then, convert that USD price to KRW (â‚©). Assume a fixed exchange rate of 1 USD = 1,400 KRW.
        
        [TARGET JSON STRUCTURE]
        {{
            "items": [
                {{
                    "item_number": "String",
                    "name": "String (Korean Name)",
                    "quantity": Integer,
                    "maker": "String",
                    "model": "String",
                    "estimated_usd": Integer (Estimated MSRP in USD. Output 0 if unknown),
                    "estimated_krw": Integer (Multiply estimated_usd by 1400 to get KRW. Output 0 if unknown),
                    "official_url": "String (Official manufacturer product URL if known. Otherwise leave empty)",
                    "search_keyword": "String (e.g., 'Maker Model price')"
                }}
            ]
        }}

        [INPUT TEXT (PROCUREMENT SPECS)]
        {chunk_text}
        
        [OUTPUT]
        Output ONLY valid JSON string.
        """
    
        response = run_ollama_chat(model_name, prompt)
        
        if response:
            try:
                cleaned = clean_json_output(response)
                parsed = json.loads(cleaned)
                
                items_list = []
                if isinstance(parsed, dict) and "items" in parsed:
                    items_list = parsed["items"]
                elif isinstance(parsed, list):
                    items_list = parsed
                
                if items_list:
                    all_items.extend(items_list)
                    print(f"      -> {len(items_list)}ê°œ í•­ëª© ì¶”ì¶œ ì„±ê³µ")
                else:
                    print(f"      -> [ì•Œë¦¼] ì¶”ì¶œëœ í•­ëª© ì—†ìŒ")

            except json.JSONDecodeError:
                print(f"      -> [ì‹¤íŒ¨] JSON íŒŒì‹± ì‹¤íŒ¨.")
            except Exception as e:
                print(f"      -> [ì˜¤ë¥˜] {e}")

    return all_items

# ==========================================
# â˜… 5. ì—‘ì…€ ì €ì¥ (ë‹¨ê°€, ì´ì•¡, ë§í¬ ì¶”ê°€ ë²„ì „)
# ==========================================
def save_to_excel(data, output_path, block_on_permission_error=True):
    df = pd.DataFrame(data)
    
    # 1. í•„ìˆ˜ ìˆ«ì ì»¬ëŸ¼ë“¤ì´ ì•„ì˜ˆ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 0ìœ¼ë¡œ ì±„ì›Œì§„ ì»¬ëŸ¼ ì„ì‹œ ìƒì„±
    for col in ['quantity', 'estimated_usd', 'estimated_krw']:
        if col not in df.columns:
            df[col] = 0
        # ì•ˆì „í•˜ê²Œ ìˆ«ìë¡œ ë³€í™˜ (ë¬¸ìì—´ì´ ì„ì—¬ìˆì–´ë„ NaNìœ¼ë¡œ ë§Œë“¤ê³  0ìœ¼ë¡œ ì±„ì›€)
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    # 2. ì´ ê¸ˆì•¡ ê³„ì‚° (ìˆ˜ëŸ‰ * ì›í™” ë‹¨ê°€)
    df['total_price'] = df['quantity'] * df['estimated_krw']
    
    # 3. ìŠ¤ë§ˆíŠ¸ ë§í¬ ìƒì„± (ê³µì‹ URL ìš°ì„ , ì—†ìœ¼ë©´ ë„¤ì´ë²„ ê²€ìƒ‰)
    def make_smart_link(row):
        # AIê°€ ê³µì‹ URLì„ ì•Œê³  ìˆë‹¤ë©´ ê³µì‹ í™ˆí˜ì´ì§€ ë§í¬ ì œê³µ
        if row.get('official_url') and str(row['official_url']).startswith('http'):
            return f'=HYPERLINK("{row["official_url"]}", "ğŸŒ ê³µì‹ í™ˆí˜ì´ì§€ ì´ë™")'
        
        # ê³µì‹ URLì´ ì—†ë‹¤ë©´ ë„¤ì´ë²„ ì‡¼í•‘ ê²€ìƒ‰ ë§í¬ ìƒì„±
        query = str(row.get('search_keyword', ''))
        if len(query) < 2 and row.get('maker') and row.get('model'):
            clean_model = re.sub(r'[^\w\s-]', '', str(row['model']))
            query = f"{row['maker']} {clean_model}"
        elif len(query) < 2:
            query = str(row.get('name', ''))
            
        search_url = f"https://search.shopping.naver.com/search/all?query={urllib.parse.quote(query)}"
        return f'=HYPERLINK("{search_url}", "ğŸ›’ ë„¤ì´ë²„ ì‡¼í•‘ ê²€ìƒ‰")'

    df['purchase_link'] = df.apply(make_smart_link, axis=1)
    
    # 4. ì—‘ì…€ ì»¬ëŸ¼ ì´ë¦„ í•œê¸€í™” ë° ìˆœì„œ ì •ë ¬
    cols = {
        'item_number': 'ë¬¼í’ˆ ë²ˆí˜¸',
        'name': 'í’ˆëª…',
        'maker': 'ì œì¡°ì‚¬(Maker)', 
        'model': 'ëª¨ë¸ëª…(Model)',
        'quantity': 'ìˆ˜ëŸ‰',
        'estimated_krw': 'ë‹¨ê°€(ì¶”ì • â‚©)',
        'estimated_usd': 'ë‹¨ê°€(ì¶”ì • $)',
        'total_price': 'ì´ ê¸ˆì•¡(â‚©)',
        'purchase_link': 'ì°¸ì¡° ë§í¬'
    }
    
    for k in cols.keys():
        if k not in df.columns: df[k] = ""
        
    df = df[list(cols.keys())]
    df.columns = [cols[c] for c in cols.keys()]
    
    # ì €ì¥ ë¡œì§
    while True:
        try:
            df.to_excel(output_path, index=False)
            print(f"\n[System] ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {output_path}")
            break
        except PermissionError:
            if not block_on_permission_error:
                raise
            print(f"\n[Warning] ì—‘ì…€ íŒŒì¼ì´ ì—´ë ¤ ìˆìŠµë‹ˆë‹¤. ë‹«ê³  ì—”í„°ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            input()
        except Exception as e:
            print(f"[Error] ì €ì¥ ì‹¤íŒ¨: {e}")
            break

# ==========================================
# â˜… ë©”ì¸: CLI ì¸ì ì²˜ë¦¬
# ==========================================
if __name__ == "__main__":
    # ìœˆë„ìš° í•œê¸€ ì¸ì½”ë”© ì„¤ì •
    if sys.platform == "win32":
        os.system('chcp 65001 > nul')

    parser = argparse.ArgumentParser()
    parser.add_argument('pdf_path', help='PDF íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model', default='gemma3:27b', help='Ollama ëª¨ë¸ëª…')
    parser.add_argument('--output', help='ì €ì¥ ê²½ë¡œ')

    try:
        args = parser.parse_args()
    except:
        print("[Error] ì¸ì íŒŒì‹± ì‹¤íŒ¨")
        sys.exit(1)

    # ë¡œê·¸ í´ë” ì„¤ì •
    log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"log_{timestamp}.txt")

    sys.stdout = DualWriter(log_file)
    print(f"[Log] ë¡œê·¸ íŒŒì¼ ìƒì„±ë¨: {log_file}")

    if not os.path.exists(args.pdf_path):
        print(f"[Error] PDF íŒŒì¼ ì—†ìŒ: {args.pdf_path}")
        sys.exit(1)

    output_file = args.output if args.output else os.path.splitext(args.pdf_path)[0] + f"_ollama.xlsx"

    # ì „ì²´ ë¡œì§ ì‹¤í–‰
    items = process_pdf(args.pdf_path, args.model)
    
    if items:
        save_to_excel(items, output_file)
    else:
        print("[System] ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")